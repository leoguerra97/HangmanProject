{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trexquant Interview Project (The Hangman Game)\n",
    "\n",
    "* Copyright Trexquant Investment LP. All Rights Reserved. \n",
    "* Redistribution of this question without written consent from Trexquant is prohibited"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instruction:\n",
    "For this coding test, your mission is to write an algorithm that plays the game of Hangman through our API server. \n",
    "\n",
    "When a user plays Hangman, the server first selects a secret word at random from a list. The server then returns a row of underscores (space separated)—one for each letter in the secret word—and asks the user to guess a letter. If the user guesses a letter that is in the word, the word is redisplayed with all instances of that letter shown in the correct positions, along with any letters correctly guessed on previous turns. If the letter does not appear in the word, the user is charged with an incorrect guess. The user keeps guessing letters until either (1) the user has correctly guessed all the letters in the word\n",
    "or (2) the user has made six incorrect guesses.\n",
    "\n",
    "You are required to write a \"guess\" function that takes current word (with underscores) as input and returns a guess letter. You will use the API codes below to play 1,000 Hangman games. You have the opportunity to practice before you want to start recording your game results.\n",
    "\n",
    "Your algorithm is permitted to use a training set of approximately 250,000 dictionary words. Your algorithm will be tested on an entirely disjoint set of 250,000 dictionary words. Please note that this means the words that you will ultimately be tested on do NOT appear in the dictionary that you are given. You are not permitted to use any dictionary other than the training dictionary we provided. This requirement will be strictly enforced by code review.\n",
    "\n",
    "You are provided with a basic, working algorithm. This algorithm will match the provided masked string (e.g. a _ _ l e) to all possible words in the dictionary, tabulate the frequency of letters appearing in these possible words, and then guess the letter with the highest frequency of appearence that has not already been guessed. If there are no remaining words that match then it will default back to the character frequency distribution of the entire dictionary.\n",
    "\n",
    "This benchmark strategy is successful approximately 18% of the time. Your task is to design an algorithm that significantly outperforms this benchmark."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-10T17:28:38.807109Z",
     "start_time": "2024-12-10T17:28:38.758822Z"
    }
   },
   "source": [
    "import json\n",
    "import time\n",
    "import re\n",
    "import requests\n",
    "import collections\n",
    "from typing import List, Counter, Dict, Tuple\n",
    "import pandas as pd\n",
    "from urllib.parse import parse_qs, urlencode, urlparse\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from transformers import AutoModelForMaskedLM, AutoTokenizer\n",
    "\n",
    "\n",
    "from urllib3.exceptions import InsecureRequestWarning\n",
    "requests.packages.urllib3.disable_warnings(InsecureRequestWarning)\n"
   ],
   "outputs": [],
   "execution_count": 41
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-10T17:28:38.554459Z",
     "start_time": "2024-12-08T22:50:33.116221Z"
    }
   },
   "source": [
    "## Load the dictionary\n",
    "def load_dictionary(path: str) -> List[str]:\n",
    "    \"\"\"\n",
    "    Load the dictionary from a file where each line represents a word.\n",
    "    \"\"\"\n",
    "    with open(path, 'r') as file:\n",
    "        words = [line.strip().lower() for line in file.readlines()]\n",
    "    return words"
   ],
   "outputs": [],
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-10T17:28:38.558085Z",
     "start_time": "2024-12-08T22:50:33.618304Z"
    }
   },
   "source": [
    "## Alphabet\n",
    "alphabet = ['a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z']\n",
    "path_to_word_dict = './words_250000_train.txt'\n",
    "\n",
    "# Load the dictionary\n",
    "words = load_dictionary(path_to_word_dict)"
   ],
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-10T17:28:38.558571Z",
     "start_time": "2024-12-08T22:50:34.663010Z"
    }
   },
   "cell_type": "code",
   "source": [
    "vowels = ['e', 'i', 'a', 'o', 'u'] # vowels ordered by freq\n",
    "from collections import defaultdict\n",
    "\n",
    "\n",
    "vowels = ['e', 'i', 'a', 'o', 'u'] # vowels ordered by freq\n",
    "# max len word in words\n",
    "max_length = max([len(word) for word in words])\n",
    "n_word_dictionary = defaultdict(list)\n",
    "\n",
    "# Optimized substring extraction\n",
    "for count in range(3, max_length + 1):\n",
    "    for word in words:\n",
    "        if len(word) >= count:\n",
    "            # Use list comprehension or generator for better performance\n",
    "            substrings = [word[i:i+count] for i in range(len(word)-count+1)]\n",
    "            n_word_dictionary[count].extend(substrings)"
   ],
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "\n",
    "# Perform preliminary data analysis\n",
    "def analyze_letter_counts(words: List[str]) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Analyze letter counts and compute the most repeated letters.\n",
    "    \"\"\"\n",
    "    letter_counts = {}\n",
    "    for word in words:\n",
    "        for letter in word:\n",
    "            letter_counts[letter] = letter_counts.get(letter, 0) + 1\n",
    "\n",
    "    total_letters = sum(letter_counts.values())\n",
    "    letter_df = pd.DataFrame(\n",
    "        {\n",
    "            \"Letter\": letter_counts.keys(),\n",
    "            \"Count\": letter_counts.values(),\n",
    "            \"Proportion\": [count / total_letters for count in letter_counts.values()],\n",
    "        }\n",
    "    ).sort_values(by=\"Count\", ascending=False)\n",
    "    return letter_df\n",
    "\n",
    "\n",
    "def compute_cooccurrence_matrix(words: List[str]) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Compute a normalized co-occurrence matrix of letters appearing together in words.\n",
    "    \"\"\"\n",
    "    # Initialize an empty DataFrame for co-occurrence\n",
    "    cooccurrence = pd.DataFrame(0, index=alphabet, columns=alphabet)\n",
    "\n",
    "    # Count co-occurrences of letters\n",
    "    for word in words:\n",
    "        unique_letters = set(word)  # Remove duplicates in the word\n",
    "        for letter1 in unique_letters:\n",
    "            for letter2 in unique_letters:\n",
    "                cooccurrence.at[letter1, letter2] += 1\n",
    "\n",
    "    # Normalize by row sums\n",
    "    #row_sums = cooccurrence.sum(axis=1)\n",
    "    #row_sums.replace(0, 1, inplace=True)  # Replace zero sums with 1 to avoid division errors\n",
    "    #correlation_matrix = cooccurrence.div(row_sums, axis=0)\n",
    "\n",
    "    return cooccurrence\n",
    "\n",
    "\n",
    "def plot_letter_distributions(letter_df):\n",
    "    \"\"\"\n",
    "    Plot letter distributions using matplotlib.\n",
    "\n",
    "    Args:\n",
    "        letter_df (pd.DataFrame): DataFrame with letter counts and proportions.\n",
    "    \"\"\"\n",
    "    # Sort by count for better visualization\n",
    "    letter_df_sorted = letter_df.sort_values(by=\"Count\", ascending=False)\n",
    "\n",
    "    # Bar plot for letter counts\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.bar(letter_df_sorted[\"Letter\"], letter_df_sorted[\"Count\"])\n",
    "    plt.title(\"Letter Frequency Distribution\")\n",
    "    plt.xlabel(\"Letters\")\n",
    "    plt.ylabel(\"Frequency\")\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "    plt.show()\n",
    "\n",
    "    # Bar plot for letter proportions\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.bar(letter_df_sorted[\"Letter\"], letter_df_sorted[\"Proportion\"])\n",
    "    plt.title(\"Letter Proportion Distribution\")\n",
    "    plt.xlabel(\"Letters\")\n",
    "    plt.ylabel(\"Proportion\")\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "    plt.show()\n"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": ""
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "\n",
    "def generate_ngrams_dict(words: List[str], n: int = 2) -> Dict:\n",
    "    \"\"\"\n",
    "    Generate n-grams from a list of words.\n",
    "    \"\"\"\n",
    "    ngram_counts = {}\n",
    "    for word in words:\n",
    "        for i in range(len(word) - n + 1):\n",
    "            ngram = word[i:i + n]\n",
    "            ngram_counts[ngram] = ngram_counts.get(ngram, 0) + 1\n",
    "    return ngram_counts\n",
    "\n",
    "# Normalize n-gram counts to probabilities\n",
    "def compute_ngram_probabilities(ngram_counts: Counter) -> Dict[str, float]:\n",
    "    \"\"\"\n",
    "    Compute probabilities from n-gram counts.\n",
    "    \"\"\"\n",
    "    total_ngrams = sum(ngram_counts.values())\n",
    "    return {ngram: count / total_ngrams for ngram, count in ngram_counts.items()}\n",
    "\n"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Generate n-grams using a normal dictionary\n",
    "bigrams_dict = generate_ngrams_dict(words, n=2)\n",
    "prob = compute_ngram_probabilities(bigrams_dict)\n",
    "\n",
    "# Analyze letter counts\n",
    "letter_analysis = analyze_letter_counts(words)\n",
    "# Plot letter distributions\n",
    "plot_letter_distributions(letter_analysis)"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Compute coocurrance matrix\n",
    "#letter_cooccurrence = compute_cooccurrence_matrix(words)\n",
    "#plt.figure(figsize=(12, 12))\n",
    "#plt.imshow(letter_cooccurrence, cmap='coolwarm')\n",
    "#plt.colorbar()\n",
    "#plt.title(\"Letter Concurrence Matrix\")\n",
    "#plt.xticks(range(len(alphabet)), alphabet)\n",
    "#plt.yticks(range(len(alphabet)), alphabet)\n",
    "# remove grid\n",
    "#plt.grid(False)\n",
    "# display value in each cell in little with only \".x\" value and small font\n",
    "#for i in range(len(alphabet)):\n",
    "#    for j in range(len(alphabet)):\n",
    "#        plt.text(j, i, f\"{letter_cooccurrence.iloc[i, j]:.01f}\", ha='center', va='center', color='black', fontsize=8)\n",
    "#plt.show()\n",
    "\n",
    "##### Bigram, Trigram Analysis\n",
    "# Function to get most common n-grams\n",
    "def most_common_ngrams(ngrams_dict: Dict[str, int], top_n: int = 20) -> List[Tuple[str, int]]:\n",
    "    \"\"\"\n",
    "    Get the most common n-grams.\n",
    "\n",
    "    Args:\n",
    "        ngrams_dict (Dict[str, int]): Dictionary of n-grams and their counts.\n",
    "        top_n (int): Number of top n-grams to return.\n",
    "\n",
    "    Returns:\n",
    "        List[Tuple[str, int]]: Sorted list of n-grams and their counts.\n",
    "    \"\"\"\n",
    "    sorted_ngrams = sorted(ngrams_dict.items(), key=lambda x: x[1], reverse=True)\n",
    "    return sorted_ngrams[:top_n]\n",
    "\n",
    "\n",
    "# Function to plot most common n-grams\n",
    "def plot_ngrams(ngrams: List[Tuple[str, int]], title: str):\n",
    "    \"\"\"\n",
    "    Plot the most common n-grams.\n",
    "\n",
    "    Args:\n",
    "        ngrams (List[Tuple[str, int]]): List of n-grams and their counts.\n",
    "        title (str): Title for the plot.\n",
    "    \"\"\"\n",
    "    labels, counts = zip(*ngrams)\n",
    "\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.bar(labels, counts)\n",
    "    plt.title(title)\n",
    "    plt.xlabel(\"N-Grams\")\n",
    "    plt.ylabel(\"Frequency\")\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# Generate N-Grams (Bigrams and Trigrams)\n",
    "trigrams_dict = generate_ngrams_dict(words, n=3)\n",
    "\n",
    "# Get and Plot Most Common N-Grams\n",
    "most_common_bigrams = most_common_ngrams(bigrams_dict, top_n=10)\n",
    "most_common_trigrams = most_common_ngrams(trigrams_dict, top_n=10)\n",
    "\n",
    "plot_ngrams(most_common_bigrams, \"Most Common Bigrams\")\n",
    "plot_ngrams(most_common_trigrams, \"Most Common Trigrams\")\n"
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-10T20:45:56.261637Z",
     "start_time": "2024-12-10T20:45:55.472165Z"
    }
   },
   "cell_type": "code",
   "source": [
    "### Average amount of vowels and consonants \n",
    "def calculate_vowel_consonant_ratios(words: List[str], vowels: List[str]) -> Dict[int, float]:\n",
    "    \"\"\"\n",
    "    Calculate the average vowel ratio (vowels/word_length) for each word length.\n",
    "\n",
    "    Args:\n",
    "        words (List[str]): List of words from the dictionary.\n",
    "        vowels (List[str]): List of vowel letters.\n",
    "\n",
    "    Returns:\n",
    "        Dict[int, float]: Mapping of word length to average vowel ratio.\n",
    "    \"\"\"\n",
    "    vowel_ratios = {}\n",
    "    length_counts = {}\n",
    "\n",
    "    for word in words:\n",
    "        word_length = len(word)\n",
    "        if word_length not in vowel_ratios:\n",
    "            vowel_ratios[word_length] = 0\n",
    "            length_counts[word_length] = 0\n",
    "\n",
    "        word_vowels = sum(1 for letter in word if letter in vowels)\n",
    "        vowel_ratios[word_length] += word_vowels\n",
    "        length_counts[word_length] += 1\n",
    "\n",
    "    # Compute average vowel ratio for each word length\n",
    "    for length in vowel_ratios:\n",
    "        vowel_ratios[length] /= length_counts[length]\n",
    "\n",
    "    return vowel_ratios\n",
    "\n",
    "\n",
    "print(calculate_vowel_consonant_ratios(words, list('aeiou')))"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{3: 0.8518855065879146, 6: 2.3428176654214217, 4: 1.4808019670890864, 5: 1.9128969309916621, 8: 3.0884014186260345, 7: 2.7040619700940343, 10: 3.8893258635402366, 9: 3.482560020707953, 11: 4.281751952953568, 12: 4.690174936736715, 13: 5.102114850262427, 15: 5.91402801765496, 14: 5.498622273249139, 20: 7.888888888888889, 17: 6.690140845070423, 16: 6.2758510976773785, 2: 0.4318181818181818, 21: 8.061224489795919, 18: 7.089639115250291, 19: 7.45124716553288, 25: 9.666666666666666, 22: 8.545454545454545, 1: 0.17647058823529413, 23: 9.428571428571429, 29: 9.5, 24: 9.222222222222221, 28: 8.0, 27: 9.0}\n"
     ]
    }
   ],
   "execution_count": 73
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### API Code"
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-10T20:03:43.966711Z",
     "start_time": "2024-12-10T20:03:43.906236Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class HangmanAPI_Combined(object):\n",
    "    def __init__(self, access_token=None, session=None, timeout=None):\n",
    "        full_dictionary_path = './words_250000_train.txt'\n",
    "        model_path = './hangman-model'\n",
    "        alpha = 0.4  # Weight for combining BERT and statistical distributions\n",
    "        self.beta = 0.1  # Weight for n-gram influence on the statistical distribution\n",
    "    \n",
    "        self.hangman_url = self.determine_hangman_url()\n",
    "        self.access_token = access_token\n",
    "        self.session = session or requests.Session()\n",
    "        self.timeout = timeout\n",
    "    \n",
    "        self.full_dictionary = self.load_dictionary(full_dictionary_path)\n",
    "        self.current_dictionary = self.full_dictionary\n",
    "        self.full_dictionary_common_letter_sorted = collections.Counter(\n",
    "            \"\".join(self.full_dictionary)\n",
    "        ).most_common()\n",
    "    \n",
    "        self.model = AutoModelForMaskedLM.from_pretrained(model_path)\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "        self.model.eval()\n",
    "    \n",
    "        # Initialize API state\n",
    "        self.guessed_letters = []\n",
    "        self.alphabet = list(\"abcdefghijklmnopqrstuvwxyz\")\n",
    "        self.letter_ids = [self.tokenizer.convert_tokens_to_ids(l) for l in self.alphabet]\n",
    "        self.letter_id_map = {lid: l for l, lid in zip(self.alphabet, self.letter_ids)}\n",
    "    \n",
    "        self.alpha = alpha # weight for combining BERT and statistical distributions\n",
    "        self.vowels = set(\"aeiou\")\n",
    "    \n",
    "    @staticmethod\n",
    "    def load_dictionary(path: str) -> List[str]:\n",
    "\n",
    "        with open(path, \"r\") as f:\n",
    "            return [line.strip() for line in f if line.strip()]\n",
    "\n",
    "\n",
    "    def filter_current_dictionary_strict(self, clean_word: str) -> List[str]:\n",
    "        # Filter dict based on exact regex match\n",
    "        regex_pattern = \"^\" + clean_word + \"$\"\n",
    "        pattern = re.compile(regex_pattern)\n",
    "        return [w for w in self.current_dictionary if len(w) == len(clean_word) and pattern.match(w)]\n",
    "\n",
    "        \n",
    "    def partial_pattern_score(self, partial_word: str, candidate_word: str) -> float:\n",
    "        \"\"\"\n",
    "        Partial matching function\n",
    "        Scoring rules (can be adjusted):\n",
    "        - +2 points for each correctly matched known letter (partial_word[i] != '.' and candidate_word[i] == that letter).\n",
    "        - -1 point if a known letter does not match.\n",
    "        - 0 points for '.' since it's unknown.\n",
    "        \n",
    "        You can add more sophisticated scoring if desired.\n",
    "        \"\"\"\n",
    "        score = 0.0\n",
    "        for p_char, c_char in zip(partial_word, candidate_word):\n",
    "            if p_char == '.': # no add\n",
    "                continue\n",
    "            else:\n",
    "                # Known letter\n",
    "                if p_char == c_char:\n",
    "                    score += 2.0\n",
    "                else:\n",
    "                    score -= 1.0\n",
    "        return score\n",
    "    \n",
    "    \n",
    "    def filter_current_dictionary_partial(self, clean_word: str, top_n: int = 50) -> List[str]:\n",
    "        scored_candidates = []\n",
    "        for w in self.current_dictionary:\n",
    "            if len(w) == len(clean_word):\n",
    "                s = self.partial_pattern_score(clean_word, w)\n",
    "                scored_candidates.append((w, s))\n",
    "        \n",
    "        scored_candidates.sort(key=lambda x: x[1], reverse=True)\n",
    "        return [w for w, s in scored_candidates[:top_n]] if scored_candidates else self.current_dictionary\n",
    "    \n",
    "    def filter_current_dictionary(self, clean_word: str, partial_threshold: int = 15):\n",
    "        \"\"\"\n",
    "        Strict filtering + partial filtering\n",
    "        Fallback to global freq if empty.\n",
    "        # To improve: optimize thresholds\n",
    "        \"\"\"\n",
    "        strict_matches = self.filter_current_dictionary_strict(clean_word)\n",
    "        if len(strict_matches) == 0: # No strict matches: use partial pattern ranking to get top candidates\n",
    "            new_dict = self.filter_current_dictionary_partial(clean_word, top_n=50)\n",
    "            if len(new_dict) == 0: #\n",
    "                new_dict = self.full_dictionary  # final fallback\n",
    "            self.current_dictionary = new_dict\n",
    "        elif len(strict_matches) < partial_threshold:\n",
    "            # Few strict matches: combine strict matches with top partial matches to broaden candidate set\n",
    "            partial_matches = self.filter_current_dictionary_partial(clean_word, top_n=50)\n",
    "            merged_set = list(set(strict_matches + partial_matches))\n",
    "            self.current_dictionary = merged_set\n",
    "        else:\n",
    "            self.current_dictionary = strict_matches  # Strict matches only\n",
    "\n",
    "    def compute_ngram_statistics(self, candidate_words: List[str], n: int = 2) -> collections.Counter:\n",
    "        \"\"\"\n",
    "        Compute n-gram frequencies for candidate words.\n",
    "        \"\"\"\n",
    "        ngram_counts = collections.Counter()\n",
    "        for w in candidate_words:\n",
    "            for i in range(len(w) - n + 1):\n",
    "                ngram = w[i:i+n]\n",
    "                ngram_counts[ngram] += 1\n",
    "        return ngram_counts\n",
    "\n",
    "    def compute_statistical_distribution(self, candidate_words: List[str]) -> Dict[str, float]:\n",
    "        \"\"\"\n",
    "        Compute statistical letter frequencies from candidate words.\n",
    "        \"\"\"\n",
    "        c = collections.Counter()\n",
    "        for word in candidate_words:\n",
    "            unique_letters = set(word)  # Count presence of each letter in each word\n",
    "            for letter in unique_letters:\n",
    "                c[letter] += 1\n",
    "\n",
    "        # Normalize to probabilities\n",
    "        total = sum(c.values())\n",
    "        if total > 0:\n",
    "            return {letter: count / total for letter, count in c.items()}\n",
    "        else:\n",
    "            return {}\n",
    "\n",
    "    def compute_bert_distribution(self, partial_word: str) -> Dict[str, float]:\n",
    "        \"\"\"\n",
    "        Compute letter probabilities using the BERT model.\n",
    "        \"\"\"\n",
    "        masked_positions = [i for i, ch in enumerate(partial_word) if ch == \".\"]\n",
    "        if not masked_positions:\n",
    "            # No unknown positions\n",
    "            return {}\n",
    "\n",
    "        letter_scores = {letter: 0.0 for letter in self.alphabet}\n",
    "        with torch.no_grad():\n",
    "            for mask_pos in masked_positions:\n",
    "                input_tokens = [\n",
    "                    \"[MASK]\" if ch == \".\" and i == mask_pos else ch\n",
    "                    for i, ch in enumerate(partial_word)\n",
    "                ]\n",
    "                masked_input_str = \" \".join(input_tokens)\n",
    "                inputs = self.tokenizer(masked_input_str, return_tensors=\"pt\", truncation=True)\n",
    "                input_ids = inputs[\"input_ids\"]\n",
    "\n",
    "                outputs = self.model(**inputs)\n",
    "                predictions = torch.softmax(outputs.logits, dim=-1)\n",
    "                mask_index = (input_ids == self.tokenizer.mask_token_id).nonzero(as_tuple=True)[1].item()\n",
    "\n",
    "                letter_probs = predictions[0, mask_index]\n",
    "                for letter, token_id in zip(self.alphabet, self.letter_ids):\n",
    "                    letter_scores[letter] += letter_probs[token_id].item()\n",
    "\n",
    "        # Normalize\n",
    "        total_score = sum(letter_scores.values())\n",
    "        if total_score > 0:\n",
    "            return {letter: score / total_score for letter, score in letter_scores.items()}\n",
    "        else:\n",
    "            return {}\n",
    "\n",
    "        \n",
    "    \n",
    "    def apply_ngram_weighting(self, stat_dist: Dict[str, float], candidate_words: List[str], clean_word: str, n=2) -> Dict[str, float]:\n",
    "        \"\"\"\n",
    "        Adjust letter distribution using n-gram statistics, scaled by self.beta.\n",
    "        \"\"\"\n",
    "        ngram_counts = self.compute_ngram_statistics(candidate_words, n)\n",
    "        total_ngrams = sum(ngram_counts.values())\n",
    "        if total_ngrams == 0:\n",
    "            return stat_dist\n",
    "    \n",
    "        known_chars = [ch for ch in clean_word if ch.isalpha()]\n",
    "    \n",
    "        # Collect letters that appear in n-grams containing known chars\n",
    "        letter_boost = collections.Counter()\n",
    "        for ngram, count in ngram_counts.items():\n",
    "            if any(kc in ngram for kc in known_chars):\n",
    "                for l in set(ngram):\n",
    "                    if l.isalpha():\n",
    "                        letter_boost[l] += count\n",
    "    \n",
    "        if letter_boost:\n",
    "            max_boost = max(letter_boost.values())\n",
    "            for ltr, val in letter_boost.items():\n",
    "                stat_dist[ltr] = stat_dist.get(ltr, 0) * (1 + self.beta * (val / max_boost))\n",
    "    \n",
    "        return stat_dist\n",
    "\n",
    "    \n",
    "    def guess(self, partial_word: str) -> str:\n",
    "        \"\"\"\n",
    "        Guess the next letter for the given partial word.\n",
    "    \n",
    "        - If more than 3 unknown letters remain, rely solely on the statistical model.\n",
    "        - If 3 or fewer unknown letters remain, combine statistical and BERT distributions.\n",
    "        \"\"\"\n",
    "        # Convert \"_ p p _ e \" to \".pp.e\"\n",
    "        clean_word = partial_word.replace(\" \", \"\").replace(\"_\", \".\")\n",
    "    \n",
    "        # Filter the dictionary based on the partial word\n",
    "        self.filter_current_dictionary(clean_word)\n",
    "    \n",
    "        # Compute statistical distribution\n",
    "        stat_dist = self.compute_statistical_distribution(self.current_dictionary)\n",
    "        # Apply n-gram weighting\n",
    "        stat_dist = self.apply_ngram_weighting(stat_dist, self.current_dictionary, clean_word, n=2)\n",
    "    \n",
    "        # Count unknown letters\n",
    "        unknown_count = clean_word.count('.')\n",
    "    \n",
    "        # If unknown_count <= 3, combine with BERT; otherwise, purely statistical\n",
    "        if unknown_count <= 3:\n",
    "            bert_dist = self.compute_bert_distribution(clean_word)\n",
    "        else:\n",
    "            # No BERT if more than 3 unknowns\n",
    "            bert_dist = {}\n",
    "    \n",
    "        combined_scores = {}\n",
    "        for letter in self.alphabet:\n",
    "            if letter not in self.guessed_letters:\n",
    "                stat_val = stat_dist.get(letter, 0.0)\n",
    "                bert_val = bert_dist.get(letter, 0.0)\n",
    "    \n",
    "                if unknown_count <= 3:\n",
    "                    # Use combined distribution\n",
    "                    score = self.alpha * bert_val + (1 - self.alpha) * stat_val\n",
    "                else:\n",
    "                    # Use only statistical distribution\n",
    "                    score = stat_val\n",
    "\n",
    "                combined_scores[letter] = score\n",
    "    \n",
    "        # Pick the best guess\n",
    "        if combined_scores:\n",
    "            guess_letter = max(combined_scores, key=combined_scores.get)\n",
    "        else:\n",
    "            # Fallback to global frequency if no candidates\n",
    "            guess_letter = next(\n",
    "                (l for l, _ in self.full_dictionary_common_letter_sorted if l not in self.guessed_letters), None\n",
    "            )\n",
    "    \n",
    "        if guess_letter:\n",
    "            self.guessed_letters.append(guess_letter)\n",
    "    \n",
    "        return guess_letter\n",
    "        \n",
    "    @staticmethod\n",
    "    def determine_hangman_url():\n",
    "        links = ['https://trexsim.com', 'https://sg.trexsim.com']\n",
    "\n",
    "        data = {link: 0 for link in links}\n",
    "\n",
    "        for link in links:\n",
    "\n",
    "            requests.get(link)\n",
    "\n",
    "            for i in range(10):\n",
    "                s = time.time()\n",
    "                requests.get(link)\n",
    "                data[link] = time.time() - s\n",
    "\n",
    "        link = sorted(data.items(), key=lambda x: x[1])[0][0]\n",
    "        link += '/trexsim/hangman'\n",
    "        return link\n",
    "\n",
    "    \n",
    "    ##########################################################\n",
    "    # You'll likely not need to modify any of the code below #\n",
    "    ##########################################################\n",
    "    \n",
    "    def build_dictionary(self, dictionary_file_location):\n",
    "        text_file = open(dictionary_file_location,\"r\")\n",
    "        full_dictionary = text_file.read().splitlines()\n",
    "        text_file.close()\n",
    "        return full_dictionary\n",
    "                \n",
    "    def start_game(self, practice=True, verbose=True):\n",
    "        # reset guessed letters to empty set and current plausible dictionary to the full dictionary\n",
    "        self.guessed_letters = []\n",
    "        self.current_dictionary = self.full_dictionary\n",
    "        self.filtered_dict = self.full_dictionary\n",
    "                         \n",
    "        response = self.request(\"/new_game\", {\"practice\":practice})\n",
    "        if response.get('status')==\"approved\":\n",
    "            game_id = response.get('game_id')\n",
    "            word = response.get('word')\n",
    "            tries_remains = response.get('tries_remains')\n",
    "            if verbose:\n",
    "                print(\"Successfully start a new game! Game ID: {0}. # of tries remaining: {1}. Word: {2}.\".format(game_id, tries_remains, word))\n",
    "            while tries_remains>0:\n",
    "                # get guessed letter from user code\n",
    "                guess_letter = self.guess(word)\n",
    "                    \n",
    "                # append guessed letter to guessed letters field in hangman object\n",
    "                self.guessed_letters.append(guess_letter)\n",
    "                if verbose:\n",
    "                    print(\"Guessing letter: {0}\".format(guess_letter))\n",
    "                    \n",
    "                try:    \n",
    "                    res = self.request(\"/guess_letter\", {\"request\":\"guess_letter\", \"game_id\":game_id, \"letter\":guess_letter})\n",
    "                except HangmanAPIError:\n",
    "                    print('HangmanAPIError exception caught on request.')\n",
    "                    continue\n",
    "                except Exception as e:\n",
    "                    print('Other exception caught on request.')\n",
    "                    raise e\n",
    "               \n",
    "                if verbose:\n",
    "                    print(\"Sever response: {0}\".format(res))\n",
    "                status = res.get('status')\n",
    "                tries_remains = res.get('tries_remains')\n",
    "                if status==\"success\":\n",
    "                    if verbose:\n",
    "                        print(\"Successfully finished game: {0}\".format(game_id))\n",
    "                    return True\n",
    "                elif status==\"failed\":\n",
    "                    reason = res.get('reason', '# of tries exceeded!')\n",
    "                    if verbose:\n",
    "                        print(\"Failed game: {0}. Because of: {1}\".format(game_id, reason))\n",
    "                    return False\n",
    "                elif status==\"ongoing\":\n",
    "                    word = res.get('word')\n",
    "        else:\n",
    "            if verbose:\n",
    "                print(\"Failed to start a new game\")\n",
    "        return status==\"success\"\n",
    "        \n",
    "    def my_status(self):\n",
    "        return self.request(\"/my_status\", {})\n",
    "    \n",
    "    def request(\n",
    "            self, path, args=None, post_args=None, method=None):\n",
    "        if args is None:\n",
    "            args = dict()\n",
    "        if post_args is not None:\n",
    "            method = \"POST\"\n",
    "\n",
    "        # Add `access_token` to post_args or args if it has not already been\n",
    "        # included.\n",
    "        if self.access_token:\n",
    "            # If post_args exists, we assume that args either does not exists\n",
    "            # or it does not need `access_token`.\n",
    "            if post_args and \"access_token\" not in post_args:\n",
    "                post_args[\"access_token\"] = self.access_token\n",
    "            elif \"access_token\" not in args:\n",
    "                args[\"access_token\"] = self.access_token\n",
    "\n",
    "        time.sleep(0.2)\n",
    "\n",
    "        num_retry, time_sleep = 50, 2\n",
    "        for it in range(num_retry):\n",
    "            try:\n",
    "                response = self.session.request(\n",
    "                    method or \"GET\",\n",
    "                    self.hangman_url + path,\n",
    "                    timeout=self.timeout,\n",
    "                    params=args,\n",
    "                    data=post_args,\n",
    "                    verify=False\n",
    "                )\n",
    "                break\n",
    "            except requests.HTTPError as e:\n",
    "                response = json.loads(e.read())\n",
    "                raise HangmanAPIError(response)\n",
    "            except requests.exceptions.SSLError as e:\n",
    "                if it + 1 == num_retry:\n",
    "                    raise\n",
    "                time.sleep(time_sleep)\n",
    "\n",
    "        headers = response.headers\n",
    "        if 'json' in headers['content-type']:\n",
    "            result = response.json()\n",
    "        elif \"access_token\" in parse_qs(response.text):\n",
    "            query_str = parse_qs(response.text)\n",
    "            if \"access_token\" in query_str:\n",
    "                result = {\"access_token\": query_str[\"access_token\"][0]}\n",
    "                if \"expires\" in query_str:\n",
    "                    result[\"expires\"] = query_str[\"expires\"][0]\n",
    "            else:\n",
    "                raise HangmanAPIError(response.json())\n",
    "        else:\n",
    "            raise HangmanAPIError('Maintype was not text, or querystring')\n",
    "\n",
    "        if result and isinstance(result, dict) and result.get(\"error\"):\n",
    "            raise HangmanAPIError(result)\n",
    "        return result\n",
    "    \n",
    "class HangmanAPIError(Exception):\n",
    "    def __init__(self, result):\n",
    "        self.result = result\n",
    "        self.code = None\n",
    "        try:\n",
    "            self.type = result[\"error_code\"]\n",
    "        except (KeyError, TypeError):\n",
    "            self.type = \"\"\n",
    "\n",
    "        try:\n",
    "            self.message = result[\"error_description\"]\n",
    "        except (KeyError, TypeError):\n",
    "            try:\n",
    "                self.message = result[\"error\"][\"message\"]\n",
    "                self.code = result[\"error\"].get(\"code\")\n",
    "                if not self.type:\n",
    "                    self.type = result[\"error\"].get(\"type\", \"\")\n",
    "            except (KeyError, TypeError):\n",
    "                try:\n",
    "                    self.message = result[\"error_msg\"]\n",
    "                except (KeyError, TypeError):\n",
    "                    self.message = result\n",
    "\n",
    "        Exception.__init__(self, self.message)"
   ],
   "outputs": [],
   "execution_count": 65
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# API Usage Examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## To start a new game:\n",
    "1. Make sure you have implemented your own \"guess\" method.\n",
    "2. Use the access_token that we sent you to create your HangmanAPI object. \n",
    "3. Start a game by calling \"start_game\" method.\n",
    "4. If you wish to test your function without being recorded, set \"practice\" parameter to 1.\n",
    "5. Note: You have a rate limit of 20 new games per minute. DO NOT start more than 20 new games within one minute."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-10T20:04:14.414140Z",
     "start_time": "2024-12-10T20:03:45.299606Z"
    }
   },
   "source": "api = HangmanAPI_Combined(access_token=\"ad5506c7195dcbf44946272eeb68a1\", timeout=2000)",
   "outputs": [],
   "execution_count": 66
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Playing practice games:\n",
    "You can use the command below to play up to 100,000 practice games."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "scrolled": true,
    "ExecuteTime": {
     "end_time": "2024-12-10T21:08:09.239112Z",
     "start_time": "2024-12-10T21:08:03.444267Z"
    }
   },
   "source": [
    "api.start_game(practice=1,verbose=True)\n",
    "[total_practice_runs,total_recorded_runs,total_recorded_successes,total_practice_successes] = api.my_status() # Get my game stats: (# of tries, # of wins)\n",
    "practice_success_rate = total_practice_successes / total_practice_runs\n",
    "print('run %d practice games out of an allotted 100,000. practice success rate so far = %.3f' % (total_practice_runs, practice_success_rate))"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully start a new game! Game ID: 094a2de5db8c. # of tries remaining: 6. Word: _ _ _ _ _ _ _ _ _ _ _ _ _ _ .\n",
      "Guessing letter: i\n",
      "Sever response: {'game_id': '094a2de5db8c', 'status': 'ongoing', 'tries_remains': 6, 'word': '_ _ _ _ _ i _ _ _ _ _ _ _ _ '}\n",
      "Guessing letter: n\n",
      "Sever response: {'game_id': '094a2de5db8c', 'status': 'ongoing', 'tries_remains': 6, 'word': '_ _ _ _ _ i _ _ _ _ n _ _ _ '}\n",
      "Guessing letter: e\n",
      "Sever response: {'game_id': '094a2de5db8c', 'status': 'ongoing', 'tries_remains': 6, 'word': '_ _ e _ _ i _ _ e _ n e _ _ '}\n",
      "Guessing letter: s\n",
      "Sever response: {'game_id': '094a2de5db8c', 'status': 'ongoing', 'tries_remains': 6, 'word': '_ _ e _ _ i _ _ e _ n e s s '}\n",
      "Guessing letter: d\n",
      "Sever response: {'game_id': '094a2de5db8c', 'status': 'ongoing', 'tries_remains': 5, 'word': '_ _ e _ _ i _ _ e _ n e s s '}\n",
      "Guessing letter: t\n",
      "Sever response: {'game_id': '094a2de5db8c', 'status': 'ongoing', 'tries_remains': 5, 'word': '_ _ e _ _ i t t e _ n e s s '}\n",
      "Guessing letter: a\n",
      "Sever response: {'game_id': '094a2de5db8c', 'status': 'ongoing', 'tries_remains': 4, 'word': '_ _ e _ _ i t t e _ n e s s '}\n",
      "Guessing letter: l\n",
      "Sever response: {'game_id': '094a2de5db8c', 'status': 'ongoing', 'tries_remains': 3, 'word': '_ _ e _ _ i t t e _ n e s s '}\n",
      "Guessing letter: o\n",
      "Sever response: {'game_id': '094a2de5db8c', 'status': 'ongoing', 'tries_remains': 3, 'word': 'o _ e _ _ i t t e _ n e s s '}\n",
      "Guessing letter: r\n",
      "Sever response: {'game_id': '094a2de5db8c', 'status': 'ongoing', 'tries_remains': 3, 'word': 'o _ e r _ i t t e r n e s s '}\n",
      "Guessing letter: v\n",
      "Sever response: {'game_id': '094a2de5db8c', 'status': 'ongoing', 'tries_remains': 3, 'word': 'o v e r _ i t t e r n e s s '}\n",
      "Guessing letter: b\n",
      "Sever response: {'game_id': '094a2de5db8c', 'status': 'success', 'tries_remains': 3, 'word': 'o v e r b i t t e r n e s s '}\n",
      "Successfully finished game: 094a2de5db8c\n",
      "run 118 practice games out of an allotted 100,000. practice success rate so far = 0.212\n"
     ]
    }
   ],
   "execution_count": 79
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-10T19:54:44.268324Z",
     "start_time": "2024-12-10T19:54:44.265174Z"
    }
   },
   "source": [],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Playing recorded games:\n",
    "Please finalize your code prior to running the cell below. Once this code executes once successfully your submission will be finalized. Our system will not allow you to rerun any additional games.\n",
    "\n",
    "Please note that it is expected that after you successfully run this block of code that subsequent runs will result in the error message \"Your account has been deactivated\".\n",
    "\n",
    "Once you've run this section of the code your submission is complete. Please send us your source code via email."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(1000):\n",
    "    print('Playing ', i, ' th game')\n",
    "    # Uncomment the following line to execute your final runs. Do not do this until you are satisfied with your submission\n",
    "    #api.start_game(practice=0,verbose=False)\n",
    "    \n",
    "    # DO NOT REMOVE as otherwise the server may lock you out for too high frequency of requests\n",
    "    time.sleep(0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## To check your game statistics\n",
    "1. Simply use \"my_status\" method.\n",
    "2. Returns your total number of games, and number of wins."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "[total_practice_runs,total_recorded_runs,total_recorded_successes,total_practice_successes] = api.my_status() # Get my game stats: (# of tries, # of wins)\n",
    "success_rate = total_recorded_successes/total_recorded_runs\n",
    "print('overall success rate = %.3f' % success_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### BERT Training script"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import json\n",
    "import requests\n",
    "import random\n",
    "import string\n",
    "import secrets\n",
    "import time\n",
    "import re\n",
    "import collections\n",
    "from typing import List, Counter, Dict, Tuple\n",
    "import pandas as pd\n",
    "from urllib.parse import parse_qs, urlencode, urlparse\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForMaskedLM, Trainer, TrainingArguments\n",
    "from torch.utils.data import Dataset\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import os\n",
    "\n",
    "from urllib3.exceptions import InsecureRequestWarning\n",
    "\n",
    "requests.packages.urllib3.disable_warnings(InsecureRequestWarning)\n",
    "\n",
    "#######################################\n",
    "# Set Random Seeds for Reproducibility\n",
    "#######################################\n",
    "seed = 42\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "def load_dictionary(path: str) -> List[str]:\n",
    "    with open(path, \"r\") as f:\n",
    "        words = [line.strip() for line in f]\n",
    "    return words\n",
    "\n",
    "path_to_word_dict = 'words_250000_train.txt'\n",
    "words = load_dictionary(path_to_word_dict)\n",
    "print(calculate_vowel_ratios(words, list('aeiou')))\n",
    "\n",
    "#######################################\n",
    "# Data Preparation Functions\n",
    "#######################################\n",
    "\n",
    "def prepare_hid_masked_dataset(\n",
    "    full_dictionary_location: str,\n",
    "    tokenizer: AutoTokenizer,\n",
    "    diversity_factor: int = 5,\n",
    "    late_game_prob: float = 0.3\n",
    ") -> List[Tuple[str, int]]:\n",
    "    \"\"\"\n",
    "    Prepare a dataset with `[HID]` and `[MASK]` tokens for fine-tuning.\n",
    "    \"\"\"\n",
    "    vowels = set('aeiou')\n",
    "\n",
    "    # Define probabilities for choosing how many consonants to hide in late-game scenarios\n",
    "    # Make sure these sum to 1.\n",
    "    hid_count_options = [0, 1, 2, 3]\n",
    "    hid_count_probs = [0.1, 0.3, 0.4, 0.2]  # Example distribution\n",
    "\n",
    "    with open(full_dictionary_location, \"r\") as f:\n",
    "        words = [line.strip() for line in f]\n",
    "\n",
    "    data = []\n",
    "    for word in words:\n",
    "        word_tokens = list(word)\n",
    "        word_length = len(word_tokens)\n",
    "        if word_length == 0:\n",
    "            continue\n",
    "\n",
    "        for _ in range(diversity_factor):\n",
    "            # Randomly select one position for `[MASK]`\n",
    "            mask_idx = random.randint(0, word_length - 1)\n",
    "            masked_word = word_tokens[:]\n",
    "            masked_word[mask_idx] = \"[MASK]\"\n",
    "\n",
    "            target_letter = word_tokens[mask_idx]\n",
    "\n",
    "            # Decide if this is a late-game scenario\n",
    "            if random.random() < late_game_prob:\n",
    "                # Late-game scenario:\n",
    "                # Select how many consonants to hide according to the given probabilities\n",
    "                chosen_hid_count = random.choices(hid_count_options, weights=hid_count_probs, k=1)[0]\n",
    "\n",
    "                # Identify consonant positions (excluding the masked one)\n",
    "                consonant_positions = [\n",
    "                    i for i, ch in enumerate(word_tokens)\n",
    "                    if i != mask_idx and ch.isalpha() and ch not in vowels\n",
    "                ]\n",
    "\n",
    "                # If not enough consonants are available, reduce the hid count\n",
    "                chosen_hid_count = min(chosen_hid_count, len(consonant_positions))\n",
    "\n",
    "                if chosen_hid_count > 0:\n",
    "                    hid_indices = random.sample(consonant_positions, chosen_hid_count)\n",
    "                    for hid_idx in hid_indices:\n",
    "                        masked_word[hid_idx] = \"[HID]\"\n",
    "\n",
    "                # Note: If chosen_hid_count = 0, we don't hide any additional consonants.\n",
    "            else:\n",
    "                # Early or mid-game scenario (the original random approach)\n",
    "                hid_indices = [\n",
    "                    i for i in range(word_length)\n",
    "                    if i != mask_idx and random.random() < 0.35\n",
    "                ]\n",
    "                for hid_idx in hid_indices:\n",
    "                    masked_word[hid_idx] = \"[HID]\"\n",
    "\n",
    "            # Convert target letter to token ID\n",
    "            target_token_id = tokenizer.convert_tokens_to_ids(target_letter)\n",
    "            if target_token_id is None or target_token_id == tokenizer.unk_token_id:\n",
    "                # If we cannot map the target letter to a token, skip this example\n",
    "                continue\n",
    "\n",
    "            # Join tokens with spaces\n",
    "            masked_input = \" \".join(masked_word)\n",
    "            data.append((masked_input, target_token_id))\n",
    "\n",
    "    return data\n",
    "\n",
    "def prepare_late_game_dataset(\n",
    "    full_dictionary_location: str,\n",
    "    tokenizer: AutoTokenizer,\n",
    "    diversity_factor: int = 5,\n",
    "    max_masks: int = 1\n",
    ") -> List[Tuple[str, List[int]]]:\n",
    "    \"\"\"\n",
    "    Prepare a dataset that simulates late-game hangman scenarios.\n",
    "    Each example will have 1 or 2 letters masked (randomly chosen),\n",
    "    and all other letters are visible or minimally hidden.\n",
    "    \"\"\"\n",
    "    with open(full_dictionary_location, \"r\") as f:\n",
    "        words = [line.strip() for line in f if line.strip()]\n",
    "\n",
    "    data = []\n",
    "    vowels = set('aeiou')  # if needed for heuristics\n",
    "    for word in words:\n",
    "        word_tokens = list(word)\n",
    "        word_length = len(word_tokens)\n",
    "        if word_length == 0:\n",
    "            continue\n",
    "\n",
    "        for _ in range(diversity_factor):\n",
    "            # Decide how many letters to mask: either 1 or 2\n",
    "            # You can weight these probabilities if desired\n",
    "            num_masks = random.choice([1, 2]) if max_masks > 1 else 1\n",
    "            if num_masks > word_length:\n",
    "                # If word too short, just mask 1 letter\n",
    "                num_masks = 1\n",
    "\n",
    "            # Randomly choose positions for masks\n",
    "            mask_positions = random.sample(range(word_length), num_masks)\n",
    "\n",
    "            masked_word = word_tokens[:]\n",
    "            target_letters = [word_tokens[pos] for pos in mask_positions]\n",
    "\n",
    "            # Replace chosen letters with [MASK]\n",
    "            for pos in mask_positions:\n",
    "                masked_word[pos] = \"[MASK]\"\n",
    "\n",
    "            # Optionally hide a few other letters minimally (late-game often means fewer unknowns)\n",
    "            # We'll do a light hid: about 10% chance on other letters\n",
    "            # This is optional and can be tweaked\n",
    "            for i in range(word_length):\n",
    "                if i not in mask_positions and random.random() < 0.1:\n",
    "                    masked_word[i] = \"[HID]\"\n",
    "\n",
    "            # Convert target letters to token IDs\n",
    "            target_token_ids = [tokenizer.convert_tokens_to_ids(t) for t in target_letters]\n",
    "            if any(t_id is None or t_id == tokenizer.unk_token_id for t_id in target_token_ids):\n",
    "                # If we can't map all target letters, skip this example\n",
    "                continue\n",
    "\n",
    "            masked_input = \" \".join(masked_word)\n",
    "            data.append((masked_input, target_token_ids))\n",
    "\n",
    "    return data\n",
    "\n",
    "#######################################\n",
    "# Custom Dataset\n",
    "#######################################\n",
    "class HangmanDataset_Single(Dataset):\n",
    "    def __init__(self, data: List[Tuple[str, int]], tokenizer: AutoTokenizer, max_length: int = 64):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.data = data\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        masked_input, target_token_id = self.data[idx]\n",
    "\n",
    "        # Tokenize input\n",
    "        encoded = self.tokenizer(\n",
    "            masked_input,\n",
    "            return_tensors=\"pt\",\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            max_length=self.max_length,\n",
    "        )\n",
    "\n",
    "        input_ids = encoded[\"input_ids\"].squeeze()\n",
    "        attention_mask = encoded[\"attention_mask\"].squeeze()\n",
    "\n",
    "        # Create labels\n",
    "        labels = input_ids.clone()\n",
    "\n",
    "        # Replace all tokens except `[MASK]` with -100\n",
    "        labels[labels != self.tokenizer.mask_token_id] = -100\n",
    "\n",
    "        # Assign target token ID to the `[MASK]`\n",
    "        mask_indices = (input_ids == self.tokenizer.mask_token_id).nonzero(as_tuple=True)[0]\n",
    "        if len(mask_indices) != 1:\n",
    "            # We expect exactly one mask token per training example\n",
    "            raise ValueError(\"Each example must have exactly one [MASK] token.\")\n",
    "        labels[mask_indices[0]] = target_token_id\n",
    "\n",
    "        return {\n",
    "            \"input_ids\": input_ids,\n",
    "            \"attention_mask\": attention_mask,\n",
    "            \"labels\": labels,\n",
    "        }\n",
    "\n",
    "class HangmanDataset_Multi(Dataset):\n",
    "    def __init__(self, data: List[Tuple[str, List[int]]], tokenizer: AutoTokenizer, max_length: int = 64):\n",
    "        \"\"\"\n",
    "        Custom Dataset for Hangman fine-tuning.\n",
    "\n",
    "        Args:\n",
    "            data (List[Tuple[str, List[int]]]): Input-output pairs (masked_word, target_token_ids).\n",
    "            tokenizer (AutoTokenizer): Tokenizer with `[HID]` and `[MASK]` tokens added.\n",
    "            max_length (int): Maximum sequence length.\n",
    "        \"\"\"\n",
    "        self.tokenizer = tokenizer\n",
    "        self.data = data\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        masked_input, target_token_ids = self.data[idx]\n",
    "\n",
    "        # Tokenize input with the custom tokenizer\n",
    "        encoded = self.tokenizer(\n",
    "            masked_input,\n",
    "            return_tensors=\"pt\",\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            max_length=self.max_length,\n",
    "        )\n",
    "\n",
    "        input_ids = encoded[\"input_ids\"].squeeze()\n",
    "        attention_mask = encoded[\"attention_mask\"].squeeze()\n",
    "\n",
    "        # Create labels\n",
    "        labels = input_ids.clone()\n",
    "\n",
    "        # Replace all tokens except `[MASK]` with -100 (ignored in loss calculation)\n",
    "        labels[:] = -100  # Initialize all tokens to -100\n",
    "\n",
    "        # Find all `[MASK]` positions and assign corresponding target token IDs\n",
    "        mask_indices = (input_ids == self.tokenizer.mask_token_id).nonzero(as_tuple=True)[0]\n",
    "        if len(mask_indices) != len(target_token_ids):\n",
    "            raise ValueError(\"Mismatch between number of [MASK] tokens and target IDs.\")\n",
    "\n",
    "        for mask_idx, target_token_id in zip(mask_indices, target_token_ids):\n",
    "            labels[mask_idx] = target_token_id\n",
    "\n",
    "        return {\n",
    "            \"input_ids\": input_ids,\n",
    "            \"attention_mask\": attention_mask,\n",
    "            \"labels\": labels,\n",
    "        }\n",
    "\n",
    "\n",
    "#######################################\n",
    "# Prepare Tokenizer and Add Tokens\n",
    "#######################################\n",
    "\n",
    "new_tokens = [\"[HID]\", \"[MASK]\"]\n",
    "alphabet_tokens = list('abcdefghijklmnopqrstuvwxyz')\n",
    "# add [HID], [MASK], and all lowercase letters as tokens\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "tokenizer.add_tokens(new_tokens, special_tokens=True)\n",
    "tokenizer.add_tokens(alphabet_tokens, special_tokens=True)\n",
    "\n",
    "#######################################\n",
    "# Prepare the Dataset\n",
    "\n",
    "#masked_data = prepare_hid_masked_dataset(\"./words_250000_train.txt\", tokenizer, diversity_factor=7)\n",
    "masked_data = prepare_late_game_dataset(\"./words_250000_train.txt\", tokenizer, diversity_factor=5)\n",
    "# Split into train and eval sets (5% eval)\n",
    "train_data, eval_data = train_test_split(masked_data, test_size=0.05, random_state=42)\n",
    "\n",
    "train_dataset = HangmanDataset_Multi(train_data, tokenizer, max_length=64)\n",
    "eval_dataset = HangmanDataset_Multi(eval_data, tokenizer, max_length=64)\n",
    "\n",
    "# Check an example\n",
    "example = train_dataset[0]\n",
    "print(f\"Input IDs: {example['input_ids']}\")\n",
    "print(f\"Attention Mask: {example['attention_mask']}\")\n",
    "print(f\"Labels: {example['labels']}\")\n",
    "\n",
    "#######################################\n",
    "# Model Preparation\n",
    "#######################################\n",
    "os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
    "model = AutoModelForMaskedLM.from_pretrained(\"distilbert-base-uncased\")\n",
    "model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./hangman-model\",\n",
    "    per_device_train_batch_size=512,\n",
    "    num_train_epochs=10,\n",
    "    save_steps=2000,\n",
    "    evaluation_strategy=\"steps\",  # Evaluate regularly\n",
    "    eval_steps=2000,  # Evaluate every 1000 steps\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=50,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset\n",
    ")\n",
    "\n",
    "\n",
    "# Resume training from the given checkpoint\n",
    "trainer.train(resume_from_checkpoint=\"./hangman-model/checkpoint-21090\")\n",
    "#trainer.train()\n",
    "\n",
    "# Save the fine-tuned model\n",
    "model.save_pretrained(\"./hangman-model\")\n",
    "tokenizer.save_pretrained(\"./hangman-model\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
